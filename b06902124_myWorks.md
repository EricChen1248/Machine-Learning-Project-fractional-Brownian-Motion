# Feature Extraction

## 1. Autoencoder

Since the number of feature is too big for a neural network, a autoencoder is built to compress and extract features. prototype: 

```python
def encoder(x, 
        actvation_list = ['relu', 'relu'], 
        layer_list = (2048, 1024),  # dimension for each layer
        name = 'temp', # to specify model file
        use_old = False): # load existing model or not
```

By default, the encoder will consist of 2 dense layers of 5000-2048-1024 and is trained with a 5000-2048-1024-2048-5000 model, with activation using 'relu'. 

## 2. Linear Regression with Square Error

We assume a particle without external force will go straight forward with constant speed, so the force generated by collision might have something to do with the difference between a linear curve and its real value. Base on this hypothesis, we generate a new dataset by the following steps on each piece of data: 

1. For the first 5000 features (MSD), do a linear regression on it and compute the variance of those 5000 time intervals. 
2. For the last 5000 features (VAC), remain unchanged. 
3. merge the two data. 

We also have a averaged version in which every 10 features of MSD will be averaged into 1, thus the 10000 feature will now become only 5500. 

### Experiments on this new dataset

#### Simple Neural Network

I first used the autoencoder on MAD and VAC respectively and merge two data into 1, and then tried this new data on a NNet which consists of 3 dense layers: 2048-50-1 for each output feature. 

The result turned out to be awful.

#### XGBoost

Used without autoencoder, the result is worse than using original data. 

## 3. Data filtered by importance computed by xgboost

We first run xgboost with original data and get its importance of each feature. Filter out features with importance under threshold, and run the filtered data again with xgboost. 

According to the experiments, the model works best when threshold is set to 0.001. 

The result is slightly improved. 

## 4. Data of 3. add predicted alpha

Since the error of alpha predicted in 3. is quite small. We tried to add it as input data based on 3. Results: 

```
Track 1: [23.19373911 16.4628059   9.20893285] 48.86547785587417
Track 2: [0.47651444 0.12814425 0.05658498] 0.6612436671219459
```

# Tuning Parameter of model Based on 4. 

The model in 4. seems to work quite well. We then further adjust the parameters for better performance. 

## 1. tree_method: auto

According to the document:

> `auto`: Use heuristic to choose the fastest method.
>
> * For small to medium dataset, exact greedy (exact) will be used.
>
> * For very large dataset, approximate algorithm (approx) will be chosen.
>
> * Because old behavior is always use exact greedy in single machine, user will get a message when approximate algorithm is chosen to notify this choice.

Unfortunately we failed to find out which tree_method we actually using. However, the error decrease significantly. 

```
Track 1: [22.835021   16.7832299   3.87269357] 43.490944474041704
Track 2: [0.44804289 0.13241683 0.02394758] 0.6044072931831186
```

## 2. reg: tweedie

Based on 1. Set the `reg` parameter to `tweedie`. 

```
Track 1: [23.70913993 17.00519485  0.98393428] 41.69826906557891
Track 2: [0.25235011 0.12481185 0.0067575 ] 0.3839194673352657
```

## 3. num_boost_round=50

Based on 2. Set the `num_boost_round` parameter to `50`. 

```
Track 1: [22.27215909 16.8850935   1.68882605] 40.846078633410684
Track 2: [0.2023435  0.12049668 0.01071442] 0.33355459538775933
```

However, it seems to be overfitted according to the online judge. 

# Try to tune the result by Neural Network

We assume the error of the results generated by xgb have some rule that can be identified and modified with neural network. Therefore, we build the network with following structure:

**\*\*TO BE COMPLETED LATER\*\***

1. compress MSD
2. compress VAC
3. merge with y predicted by xgb
4. merge into out_1
5. merge with y predicted by xgb

local:

```
E_in:
Track 1: [20.50275032 14.17072206  0.77696573] 35.45043810504901
Track 2: [0.22421029 0.1032097  0.00529571] 0.33271569275001855

E_out:
Track 1: [26.01761568 17.75526591  0.90936219] 44.68224378472564
Track 2: [0.31223738 0.13294674 0.00624992] 0.4514340477842021
```

online:

```
(public/private)
Track 1: 47.313945	48.097041
Track2: 0.805689	0.613215
```



# Blending